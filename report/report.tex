\documentclass[journal]{IEEEtran}

\newtheorem{defn}{Definition}[section]
% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% can use linebreaks \\ within to get better formatting as desired
\title{Prominent users detection}

\author{Ildar~Nurgaliev}


% The paper headers
\markboth{Dainfos lab,~interim report, No.~1, August~2015}%
{Shell \MakeLowercase{\textit{et al.}}: Bare Demo of IEEEtran.cls for Journals}

% make the title area
\maketitle

\begin{IEEEkeywords}
CloSpam algoritm for Frequent Item Mining
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle

\section{Abstract}
In this paper, we study the problem of mining for frequent trajectories, which is crucial in many application scenar- ios, such as vehicle traffic management, hand-off in cellular networks, supply chain management. We approach this problem as that of mining for frequent sequential patterns. Our approach consists of a partitioning strategy for incoming streams of trajectories in order to reduce the trajectory size and represent trajectories as strings. We mine frequent trajectories using a sliding windows approach combined with a counting algorithm that allows us to promptly update the frequency of patterns. In order to make counting really efficient, we represent frequent trajectories by prime numbers, whereby the Chinese reminder theorem can then be used to expedite the computation.

\hfill August 6, 2015

\section{Introduction}
In this paper, we ad1dress the problem of extracting frequent patterns from trajectory data streams. Due to its many applications and technical challenges, the problem of extracting frequent patterns has received a great deal of at- tention since the time it was originally introduced for trans- actional data [1, 4]. For trajectory data the problem was studied in [3,12]. The challenge posed by data stream sys- tems and data stream mining is that, in many applications, data must be processed continuously, either because of real time requirements or simply because the stream is too mas- sive for a store-now and process-later approach. However, mining of data streams brings many challenges not encoun- tered in database mining, because of the real-time response requirement and the presence of bursty arrivals and concept shifts (i.e., changes in the statistical properties of data). In order to cope with such challenges, the continuous stream is often divided into windows, thus reducing the size of the data that need to be stored and mined. This allows de- tecting concept drifts/shifts by monitoring changes between subsequent windows. Even so, frequent pattern mining over such large windows remains a computationally challenging.

\section{Problem}
The sequenctial pattern mining algorithms developed so far have good performance in databases consisting of short frequent sequences. Unfortunately, when mining long frequent sequences, or when using very low support thresholds, the performance of such agorithms ften degrades dramatically. The best approach to solve this problem is to mining frequent closed patterns and do post-prunning while searching in order to reduce search space.

\section{Method}
According to authors of the paper \cite{CloSpan} instead of mining the complete set of frequent subsequences we mine frequent. There theu propose CloSpan algorithm for such problem. Algorithm's work is based on lexicographic Sequence Tree conscruction:
\begin{enumerate}
\item each node in the tree corresponds to a sequence, and the root is a {\it null} sequence;
\item if a parent node corresponds to a sequence {\it s}, its child is either an itemset-extension of {\it s}, or a sequence-extension of {\it s};
\item the left sibling is less than the right sibling in sequence lexicographic order.
\end{enumerate}
Chosen data structure proposes effective passing and prunning search space. On the basis of Lexicographic Sequence Tree was induced Lemma 1 (Common Prefix) , Lemma 2 (Partial Order), Theorem 1 (Equivalence of Projected Databases) and Lemma 3 (Early Termination by Equivalence). As a corollary it gives effective prunning methods: Corollary 1 (Backward Sub-Pattern) and else the same one Corollary 2 (Backward Super-Pattern). As a result it proposes 2 main steps CloSpan do divide mining process.
\begin{enumerate}
\item Generated the {\it LS} set, a superset of closed frequent sequences, and stores it in a prefix sequence lattice;
\item it does post-pruning to eliminate non-closed sequences.
\end{enumerate}
There is should be mentioned that in implementation there is used some hints in order to increase performance of algorithm such as Hash index on the size of projected database that speed up check out on Theorem 1. Futhermore the hash key depends not only from the size, also it depends on the position of the projected database. So the performance will not degrade  in small range of possible sizes of projected databases.

\section{Experiment}
Developing and testing...

\section{Conclusion}
This approach gives benefits such as:
\begin{itemize}
  \item can mine really long sequences;
  \item produce significantly less number of discovered frequence sequences.
\end{itemize}

More detailed benefits are:
\begin{itemize}
\item Solve closed sequential pattern mining problem;
\item {\it CloSpan} outperforms {\it PrefixSpan} by more than one order of magnitude;
\item capable of mining longer frequent sequences in a large data set with low min\_sup;
\item it does not modify the frequent pattern mining algorithm: it only defines the early termination condition of search branch;
\item this method can be extended to other existing sequential pattern mining algorithms (SPADE, SPAM).
\end{itemize}

\bibliographystyle{unsrt}
\bibliography{references}
\end{document}